{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ccf474c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5749e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Only first time I need that\n",
    "'''\n",
    "# Import library\n",
    "import gdown\n",
    "\n",
    "# Gdrive link\n",
    "gdrive_url = \"https://drive.google.com/file/d/1ECfl3dtYyfivY8kYPq7RHUBTjC-2vf61/view?usp=share_link\"\n",
    "\n",
    "# ID of the file you want to download\n",
    "file_id = gdrive_url.split('/')[-2]\n",
    "file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download format\n",
    "prefix = 'https://drive.google.com/uc?/export=download&id='\n",
    "\n",
    "# Output you want\n",
    "url = prefix+file_id\n",
    "output = \"../datasets/waste-material-dataset.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9a7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(file_url, output):\n",
    "    try:\n",
    "        # Download the file\n",
    "        gdown.download(file_url, output)\n",
    "        print('File downloaded successfully.')\n",
    "    except Exception as e:\n",
    "        print('An error occurred:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dda5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file(url, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Only first time I need that\n",
    "'''\n",
    "# Extract zip file\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def extract_zip_files(file_name, unzip_path):\n",
    "    try:\n",
    "        with ZipFile(file_name, 'r') as file:\n",
    "            print('Extract all the files...')\n",
    "            file.extractall(path=unzip_path)\n",
    "            print(f\"Successfully extracted to {unzip_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Extracting file error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Only first time I need that\n",
    "'''\n",
    "file_name = \"../datasets/waste-material-dataset.zip\"\n",
    "unzip_path = \"../datasets\"\n",
    "\n",
    "extract_zip_files(file_name, unzip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76480d77",
   "metadata": {},
   "source": [
    "# I made a few strategies to deal with the dataset.\n",
    "\n",
    "### 1. Understanding the Dataset\n",
    "### 2. EDA\n",
    "### 3. Image Processing on Bounding Box\n",
    "### 4. Model Traning / Apply Transfer Learning\n",
    "### 5. Model Validation, and Evaluation\n",
    "### 6. Model Selection\n",
    "### 7. Model Testing & Export\n",
    "### 8. Model Monitoring and Maintaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be8a08d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Start here\n",
    "'''\n",
    "# Importing libraries\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Disable python warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# CV & image processing libraries\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image and label directories\n",
    "train_image_dir = \"../datasets/train/images\"\n",
    "train_label_dir = \"../datasets/train/labels\"\n",
    "valid_image_dir = \"../datasets/valid/images\"\n",
    "valid_label_dir = \"../datasets/valid/labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6826d511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': '../datasets/train/images',\n",
       " 'val': '../datasets/valid/images',\n",
       " 'nc': 13,\n",
       " 'names': ['banana',\n",
       "  'chilli',\n",
       "  'drinkcan',\n",
       "  'drinkpack',\n",
       "  'foodcan',\n",
       "  'lettuce',\n",
       "  'paperbag',\n",
       "  'plasticbag',\n",
       "  'plasticbottle',\n",
       "  'plasticcontainer',\n",
       "  'sweetpotato',\n",
       "  'teabag',\n",
       "  'tissueroll']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load yaml file\n",
    "import yaml\n",
    "\n",
    "with open(\"../datasets/data.yaml\", \"r\") as f:\n",
    "    dataset = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c67bb7d",
   "metadata": {},
   "source": [
    "## 1. Understanding the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def explain_dataset(image_dir, label_dir, class_names, num_classes):\n",
    "    \n",
    "    # Storage variables\n",
    "    image_count = 0\n",
    "    label_count = 0\n",
    "    image_not_count = 0\n",
    "\n",
    "    images_per_class = defaultdict(int)\n",
    "    instances_per_class = defaultdict(int)\n",
    "    objects_per_image = defaultdict(int)\n",
    "\n",
    "    missing_label_files = []\n",
    "    empty_label_files = []\n",
    "    images_without_annotations = []\n",
    "\n",
    "    total_bboxes = 0\n",
    "    bbox_areas = []\n",
    "    \n",
    "    for img_file in os.listdir(image_dir):\n",
    "        if not img_file.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "            image_not_count += 1\n",
    "            continue\n",
    "        \n",
    "        image_count += 1\n",
    "        image_name = os.path.splitext(img_file)[0]\n",
    "        label_file = os.path.join(label_dir, f\"{image_name}.txt\")\n",
    "        \n",
    "        # Check if label file exists\n",
    "        if not os.path.exists(label_file):\n",
    "            missing_label_files.append(img_file)\n",
    "            images_without_annotations.append(img_file)\n",
    "            continue\n",
    "        else:\n",
    "            label_count += 1\n",
    "        \n",
    "        # Read annotate \n",
    "        with open(label_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        if lines == [''] or len(lines) == 0:\n",
    "            empty_label_files.append(img_file)\n",
    "            images_without_annotations.append(img_file)\n",
    "            continue\n",
    "        \n",
    "        # Count objects\n",
    "        objects_per_image[img_file] = len(lines)\n",
    "        total_bboxes += len(lines)\n",
    "        \n",
    "        # Count instances\n",
    "        for line in lines:\n",
    "            cls, x, y, w, h = map(float, line.split())\n",
    "            cls = int(cls)\n",
    "            \n",
    "            instances_per_class[cls] += 1\n",
    "            images_per_class[cls] += 1\n",
    "            \n",
    "            # Calculate bbox area\n",
    "            bbox_area = w * h\n",
    "            bbox_areas.append(bbox_area)\n",
    "        \n",
    "    # DISPLAY STATISTICS\n",
    "    print(\"\\n====== DATASET STATISTICS ======\\n\")\n",
    "    print(\"Total images:\", image_count)\n",
    "    print(\"Total label files:\", label_count)\n",
    "    print(\"Total bounding boxes:\", total_bboxes)\n",
    "    print(\"Average bounding-box size (normalized area):\", \n",
    "        np.mean(bbox_areas) if bbox_areas else 0)\n",
    "\n",
    "    print(\"\\n=== IMAGES PER CLASS ===\")\n",
    "    for cls in range(num_classes):\n",
    "        print(f\"{cls} ({class_names[cls]}): {images_per_class[cls]} images\")\n",
    "\n",
    "    print(\"\\n=== INSTANCES PER CLASS ===\")\n",
    "    for cls in range(num_classes):\n",
    "        print(f\"{cls} ({class_names[cls]}): {instances_per_class[cls]} instances\")\n",
    "\n",
    "    print(\"\\nImages without annotation labels:\", len(images_without_annotations))\n",
    "    print(\"Missing label files:\", len(missing_label_files))\n",
    "    print(\"Empty label files:\", len(empty_label_files))\n",
    "    print(\"Images other formats:\", image_not_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316acd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = dataset[\"names\"]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "explain_dataset(train_image_dir, train_label_dir, class_names, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd20280",
   "metadata": {},
   "source": [
    "## Train Dataset Observations:\n",
    "\n",
    "- I'm using YOLO format to understand and analyze the dataset.\n",
    "- The dataset contains two splits: train and valid.\n",
    "- Each split contains two folders:\n",
    "    - images/ → original images\n",
    "    - labels/ → YOLO annotation files\n",
    "- Each annotation file contains lines in the format:\n",
    "- class x_center y_center width height, all values normalized.\n",
    "\n",
    "## Understanding Images and Annotations\n",
    "\n",
    "- Each image may contain one or more annotations, but in this dataset most images contain exactly one object.\n",
    "- Each annotation is linked to only one image through the same filename.\n",
    "- Class IDs range from 0 to 12, mapped to 13 categories like \"banana\", \"chilli\", \"drinkcan\", etc.\n",
    "- The dataset appears to have one instance per image (based on total images = 919 and total instances = 1180, with very few multi-object images).\n",
    "\n",
    "## Parsed and Computed Statistics\n",
    "\n",
    "- Loaded all images and their corresponding YOLO annotation files.\n",
    "\n",
    "- Counted:\n",
    "    - Total number of images\n",
    "    - Total number of labels (same count → perfectly consistent)\n",
    "    - Total number of bounding boxes (1180)\n",
    "    - Bounding-box sizes\n",
    "    - Objects per image\n",
    "    - Instances per class\n",
    "    - Images per class\n",
    "- Verified data consistency:\n",
    "    - No missing labels\n",
    "    - No empty label files\n",
    "    - No images without annotations\n",
    "    - No images are any other format\n",
    "- Bounding box statistics:\n",
    "    - Average normalized bbox area = 0.3129\n",
    "    (meaning boxes are generally large relative to image size)\n",
    "\n",
    "## Class Distribution\n",
    "\n",
    "- Extracted category IDs and counted occurrences of each class.\n",
    "- The dataset has 13 object categories:\n",
    "    - The largest class is sweetpotato (120 instances).\n",
    "    - The smallest class is chilli (71 instances).\n",
    "- Class distribution is relatively balanced, with no extremely rare classes.\n",
    "\n",
    "## Counting Objects Per Image\n",
    "\n",
    "- Computed number of objects per image across both splits.\n",
    "- Most images contain a single object, since total instances (1180) is only slightly higher than total images (919).\n",
    "- The average number of objects per image ≈ 1.28.\n",
    "\n",
    "## Conclusions:\n",
    "\n",
    "- The dataset contains two YOLO-format splits: train and valid.\n",
    "- Directory structure is consistent, containing:\n",
    "    - images/\n",
    "    - labels/\n",
    "    - data.yaml defining the 13 class names.\n",
    "- All images have corresponding label files:\n",
    "    - 0 missing labels\n",
    "    - 0 empty labels\n",
    "    - 0 unannotated images\n",
    "    - 0 image are other format\n",
    "    - This indicates a clean and well-prepared dataset.\n",
    "- Each annotation follows YOLO format with class IDs and normalized bounding boxes.\n",
    "- The dataset contains:\n",
    "    - 919 images\n",
    "    - 919 label files\n",
    "    - 1180 object instances\n",
    "- Class distribution is balanced overall:\n",
    "    - Highest: sweetpotato (120)\n",
    "    - Lowest: chilli (71)\n",
    "- Bounding boxes are generally large, with an average normalized area of ~0.31.\n",
    "- On average, each image contains 1.28 objects, meaning the dataset is primarily single-object per image.\n",
    "- Category names match the 13 waste/food-related classes:\n",
    "banana, chilli, drinkcan, drinkpack, foodcan, lettuce, paperbag, plasticbag, plasticbottle, plasticcontainer, sweetpotato, teabag, tissueroll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_dataset(valid_image_dir, valid_label_dir, class_names, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f6779f",
   "metadata": {},
   "source": [
    "## Valid Dataset Observations:\n",
    "\n",
    "- I'm using YOLO format to understand and analyze the dataset.\n",
    "- The dataset contains two splits: train and valid.\n",
    "- Each split contains two folders:\n",
    "    - images/ → original images\n",
    "    - labels/ → YOLO annotation files\n",
    "- Each annotation file contains lines in the format:\n",
    "- class x_center y_center width height, all values normalized.\n",
    "\n",
    "## Understanding Images and Annotations\n",
    "\n",
    "- Each image may contain one or more annotations, but in this dataset most images contain exactly one object.\n",
    "- Each annotation is linked to only one image through the same filename.\n",
    "- Class IDs range from 0 to 12, mapped to 13 categories like \"banana\", \"chilli\", \"drinkcan\", etc.\n",
    "- The dataset appears to have one instance per image (based on total images = 459 and total instances = 702, with very few multi-object images).\n",
    "\n",
    "## Parsed and Computed Statistics\n",
    "\n",
    "- Loaded all images and their corresponding YOLO annotation files.\n",
    "\n",
    "- Counted:\n",
    "    - Total number of images\n",
    "    - Total number of labels (same count → perfectly consistent)\n",
    "    - Total number of bounding boxes (702)\n",
    "    - Bounding-box sizes\n",
    "    - Objects per image\n",
    "    - Instances per class\n",
    "    - Images per class\n",
    "- Verified data consistency:\n",
    "    - No missing labels\n",
    "    - No empty label files\n",
    "    - No images without annotations\n",
    "    - No images are any other format\n",
    "- Bounding box statistics:\n",
    "    - Average normalized bbox area = 0.2643\n",
    "    (meaning boxes are generally large relative to image size)\n",
    "\n",
    "## Class Distribution\n",
    "\n",
    "- Extracted category IDs and counted occurrences of each class.\n",
    "- The dataset has 13 object categories:\n",
    "    - The largest class is tissueroll (63 instances).\n",
    "    - The smallest class is plasticbag (40 instances).\n",
    "- Class distribution is relatively balanced, with no extremely rare classes.\n",
    "\n",
    "## Counting Objects Per Image\n",
    "\n",
    "- Computed number of objects per image across both splits.\n",
    "- Most images contain a single object, since total instances (702) is only slightly higher than total images (459).\n",
    "- The average number of objects per image ≈ 1.52.\n",
    "\n",
    "## Conclusions:\n",
    "\n",
    "- The dataset contains two YOLO-format splits: train and valid.\n",
    "- Directory structure is consistent, containing:\n",
    "    - images/\n",
    "    - labels/\n",
    "    - data.yaml defining the 13 class names.\n",
    "- All images have corresponding label files:\n",
    "    - 0 missing labels\n",
    "    - 0 empty labels\n",
    "    - 0 unannotated images\n",
    "    - 0 image are other format\n",
    "    - This indicates a clean and well-prepared dataset.\n",
    "- Each annotation follows YOLO format with class IDs and normalized bounding boxes.\n",
    "- The dataset contains:\n",
    "    - 459 images\n",
    "    - 459 label files\n",
    "    - 702 object instances\n",
    "- Class distribution is balanced overall:\n",
    "    - Highest: tissueroll (63)\n",
    "    - Lowest: plasticbag (40)\n",
    "- Bounding boxes are generally large, with an average normalized area of ~0.31.\n",
    "- On average, each image contains 1.28 objects, meaning the dataset is primarily single-object per image.\n",
    "- Category names match the 13 waste/food-related classes:\n",
    "banana, chilli, drinkcan, drinkpack, foodcan, lettuce, paperbag, plasticbag, plasticbottle, plasticcontainer, sweetpotato, teabag, tissueroll."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aefe37",
   "metadata": {},
   "source": [
    "## 2. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355c9ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(image_dir, label_dir, class_names, num_classes):\n",
    "    \n",
    "    # Storage variables\n",
    "    image_info = []\n",
    "    annotations = []\n",
    "    image_not_count = 0\n",
    "\n",
    "    objects_per_image = Counter()\n",
    "    instances_per_class = Counter()\n",
    "    co_occurrence = defaultdict(lambda: Counter())\n",
    "\n",
    "    missing_labels = []\n",
    "    empty_annotations = []\n",
    "    bbox_areas = []\n",
    "    bbox_widths = []\n",
    "    bbox_heights = []\n",
    "    bbox_centers = []\n",
    "    \n",
    "    for img_file in os.listdir(image_dir):\n",
    "        if not img_file.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "            image_not_count += 1\n",
    "            continue\n",
    "        \n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        label_path = os.path.join(label_dir, img_file.rsplit(\".\",1)[0] + \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722ea700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# YOLO DATASET EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CONFIG\n",
    "# ------------------------------------------------------------\n",
    "dataset_path = \"dataset\"   # root folder\n",
    "splits = [\"train\", \"valid\"]\n",
    "\n",
    "class_names = [\n",
    "    'banana','chilli','drinkcan','drinkpack','foodcan','lettuce',\n",
    "    'paperbag','plasticbag','plasticbottle','plasticcontainer',\n",
    "    'sweetpotato','teabag','tissueroll'\n",
    "]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STORAGE\n",
    "# ------------------------------------------------------------\n",
    "image_info = []           # (file_path, width, height)\n",
    "annotations = []          # (image_path, class, x_center, y_center, w, h)\n",
    "objects_per_image = Counter()\n",
    "instances_per_class = Counter()\n",
    "co_occurrence = defaultdict(lambda: Counter())\n",
    "\n",
    "missing_labels = []\n",
    "empty_annotations = []\n",
    "bbox_areas = []\n",
    "bbox_widths = []\n",
    "bbox_heights = []\n",
    "bbox_centers = []\n",
    "\n",
    "# =============================================================\n",
    "# PARSE DATASET\n",
    "# =============================================================\n",
    "print(\"Parsing dataset...\")\n",
    "\n",
    "for split in splits:\n",
    "    img_dir = os.path.join(dataset_path, split, \"images\")\n",
    "    label_dir = os.path.join(dataset_path, split, \"labels\")\n",
    "    \n",
    "    for img_file in tqdm(os.listdir(img_dir)):\n",
    "        if not img_file.lower().endswith((\".jpg\",\".png\",\".jpeg\")):\n",
    "            continue\n",
    "        \n",
    "        img_path = os.path.join(img_dir, img_file)\n",
    "        label_path = os.path.join(label_dir, img_file.rsplit(\".\",1)[0] + \".txt\")\n",
    "\n",
    "        # Load image resolution\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        h, w = img.shape[:2]\n",
    "        image_info.append((img_path, w, h))\n",
    "\n",
    "        # Check annotations\n",
    "        if not os.path.exists(label_path):\n",
    "            missing_labels.append(img_path)\n",
    "            continue\n",
    "        \n",
    "        with open(label_path, \"r\") as f:\n",
    "            lines = [l.strip() for l in f.readlines() if l.strip()]\n",
    "\n",
    "        if len(lines) == 0:\n",
    "            empty_annotations.append(img_path)\n",
    "            continue\n",
    "\n",
    "        objects_per_image[img_path] = len(lines)\n",
    "\n",
    "        # Track which classes in this image\n",
    "        present_classes = set()\n",
    "\n",
    "        for line in lines:\n",
    "            cls, xc, yc, bw, bh = map(float, line.split())\n",
    "            cls = int(cls)\n",
    "\n",
    "            annotations.append((img_path, cls, xc, yc, bw, bh))\n",
    "            instances_per_class[cls] += 1\n",
    "            present_classes.add(cls)\n",
    "\n",
    "            bbox_areas.append(bw * bh)\n",
    "            bbox_widths.append(bw)\n",
    "            bbox_heights.append(bh)\n",
    "            bbox_centers.append((xc, yc))\n",
    "\n",
    "        # record co-occurrence\n",
    "        for c1 in present_classes:\n",
    "            for c2 in present_classes:\n",
    "                if c1 != c2:\n",
    "                    co_occurrence[c1][c2] += 1\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# 1. CLASS DISTRIBUTION AND PERCENTAGE ANALYSIS\n",
    "# =================================================================\n",
    "print(\"\\n=== CLASS DISTRIBUTION ===\")\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x=list(class_names), y=[instances_per_class[i] for i in range(num_classes)], palette=\"viridis\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Instances per Class\")\n",
    "plt.show()\n",
    "\n",
    "# Percentage\n",
    "total_objects = sum(instances_per_class.values())\n",
    "percentages = {class_names[i]: (instances_per_class[i]/total_objects*100) for i in range(num_classes)}\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.pie(percentages.values(), labels=percentages.keys(), autopct=\"%1.1f%%\")\n",
    "plt.title(\"Class Percentage Distribution\")\n",
    "plt.show()\n",
    "\n",
    "print(percentages)\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# 2. OBJECT FREQUENCY PER IMAGE\n",
    "# =================================================================\n",
    "obj_counts = list(objects_per_image.values())\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(obj_counts, bins=10, kde=True)\n",
    "plt.title(\"Objects Per Image Distribution\")\n",
    "plt.xlabel(\"Number of objects\")\n",
    "plt.show()\n",
    "\n",
    "avg_objects = np.mean(obj_counts)\n",
    "print(\"Average objects per image:\", avg_objects)\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# 3. BBOX SIZE & LOCATION DISTRIBUTION\n",
    "# =================================================================\n",
    "\n",
    "# Area distribution\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(bbox_areas, bins=20, kde=True)\n",
    "plt.title(\"Bounding Box Area Distribution (normalized)\")\n",
    "plt.show()\n",
    "\n",
    "# Width distribution\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(bbox_widths, bins=20, kde=True)\n",
    "plt.title(\"Bounding Box Width Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Height distribution\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(bbox_heights, bins=20, kde=True)\n",
    "plt.title(\"Bounding Box Height Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Heatmap of bbox centers\n",
    "xs = [c[0] for c in bbox_centers]\n",
    "ys = [c[1] for c in bbox_centers]\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.kdeplot(x=xs, y=ys, fill=True, cmap=\"Reds\")\n",
    "plt.title(\"Bounding Box Center Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# 4. IMAGE RESOLUTION ANALYSIS\n",
    "# =================================================================\n",
    "widths = [w for _, w, _ in image_info]\n",
    "heights = [h for _, _, h in image_info]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(widths, heights, alpha=0.4)\n",
    "plt.xlabel(\"Width\")\n",
    "plt.ylabel(\"Height\")\n",
    "plt.title(\"Image Resolution Distribution\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Average resolution:\", (np.mean(widths), np.mean(heights)))\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# 5. ANNOTATION QUALITY CHECKING (RANDOM SAMPLES)\n",
    "# =================================================================\n",
    "def show_random_samples(n=5):\n",
    "    import random\n",
    "    samples = random.sample(annotations, n)\n",
    "    \n",
    "    for img_path, cls, xc, yc, bw, bh in samples:\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        x1 = int((xc - bw/2) * w)\n",
    "        y1 = int((yc - bh/2) * h)\n",
    "        x2 = int((xc + bw/2) * w)\n",
    "        y2 = int((yc + bh/2) * h)\n",
    "\n",
    "        cv2.rectangle(img, (x1,y1), (x2,y2), (255,0,0), 2)\n",
    "        cv2.putText(img, class_names[cls], (x1,y1-5), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,0,0), 2)\n",
    "        \n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\nShowing random annotated images...\")\n",
    "show_random_samples(5)\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# 6. DATASET BALANCE BETWEEN SPLITS\n",
    "# =================================================================\n",
    "split_instances = {s: Counter() for s in splits}\n",
    "\n",
    "for img_path, cls, *_ in annotations:\n",
    "    if \"train\" in img_path:\n",
    "        split_instances[\"train\"][cls] += 1\n",
    "    else:\n",
    "        split_instances[\"valid\"][cls] += 1\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "x = np.arange(num_classes)\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, [split_instances[\"train\"][i] for i in range(num_classes)], width, label=\"Train\")\n",
    "plt.bar(x + width/2, [split_instances[\"valid\"][i] for i in range(num_classes)], width, label=\"Valid\")\n",
    "\n",
    "plt.xticks(x, class_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.title(\"Class Balance: Train vs Valid\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# 7. OUTLIER DETECTION\n",
    "# =================================================================\n",
    "# Bbox size outliers\n",
    "areas = np.array(bbox_areas)\n",
    "mean_area = np.mean(areas)\n",
    "std_area = np.std(areas)\n",
    "outliers = np.where((areas > mean_area + 3*std_area) | (areas < mean_area - 3*std_area))[0]\n",
    "\n",
    "print(\"\\nBounding box outliers:\", len(outliers))\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(x=areas)\n",
    "plt.title(\"Boxplot of Normalized BBox Area\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# 8. CORRELATION / CO-OCCURRENCE ANALYSIS\n",
    "# =================================================================\n",
    "matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "for c1 in co_occurrence:\n",
    "    for c2 in co_occurrence[c1]:\n",
    "        matrix[c1][c2] = co_occurrence[c1][c2]\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(matrix, xticklabels=class_names, yticklabels=class_names, cmap=\"Blues\", annot=True)\n",
    "plt.title(\"Class Co-occurrence Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71731e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
