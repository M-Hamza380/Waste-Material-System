{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ccf474c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d5749e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1y50gri4vFI3y1WZTEly1nlBA5pih-DPL'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Only first time I need that\n",
    "'''\n",
    "# Import library\n",
    "import gdown\n",
    "\n",
    "# Gdrive link\n",
    "gdrive_url = \"https://drive.google.com/file/d/1y50gri4vFI3y1WZTEly1nlBA5pih-DPL/view?usp=sharing\"\n",
    "\n",
    "# ID of the file you want to download\n",
    "file_id = gdrive_url.split('/')[-2]\n",
    "file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download format\n",
    "prefix = 'https://drive.google.com/uc?/export=download&id='\n",
    "\n",
    "# Output you want\n",
    "url = prefix+file_id\n",
    "output = \"../datasets/waste-material-dataset.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c9a7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(file_url, output):\n",
    "    try:\n",
    "        # Download the file\n",
    "        gdown.download(file_url, output)\n",
    "        print('File downloaded successfully.')\n",
    "    except Exception as e:\n",
    "        print('An error occurred:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1dda5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?/export=download&id=1y50gri4vFI3y1WZTEly1nlBA5pih-DPL\n",
      "From (redirected): https://drive.google.com/uc?%2Fexport=download&id=1y50gri4vFI3y1WZTEly1nlBA5pih-DPL&confirm=t&uuid=4751ca34-0048-4bf8-b8b5-f3186878d7ac\n",
      "To: h:\\Git_Projects\\Waste-Material-System\\src\\backend\\datasets\\waste-material-dataset.zip\n",
      "100%|██████████| 40.2M/40.2M [00:18<00:00, 2.12MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "download_file(url, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Only first time I need that\n",
    "'''\n",
    "# Extract zip file\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def extract_zip_files(file_name, unzip_path):\n",
    "    try:\n",
    "        with ZipFile(file_name, 'r') as file:\n",
    "            print('Extract all the files...')\n",
    "            file.extractall(path=unzip_path)\n",
    "            print(f\"Successfully extracted to {unzip_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Extracting file error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract all the files...\n",
      "Successfully extracted to ../datasets\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Only first time I need that\n",
    "'''\n",
    "file_name = \"../datasets/waste-material-dataset.zip\"\n",
    "unzip_path = \"../datasets\"\n",
    "\n",
    "extract_zip_files(file_name, unzip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76480d77",
   "metadata": {},
   "source": [
    "# I developed several strategies to handle the dataset.\n",
    "\n",
    "### 1. Comprehending the Dataset\n",
    "### 2. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8a08d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Start here\n",
    "'''\n",
    "# Importing libraries\n",
    "import numpy as np\n",
    "import os, cv2, random\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "# Disable python warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image and label directories\n",
    "train_image_dir = \"../datasets/data/train/images\"\n",
    "train_label_dir = \"../datasets/data/train/labels\"\n",
    "valid_image_dir = \"../datasets/data/valid/images\"\n",
    "valid_label_dir = \"../datasets/data/valid/labels\"\n",
    "test_image_dir = \"../datasets/data/test/images\"\n",
    "test_label_dir = \"../datasets/data/test/labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6826d511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load yaml file\n",
    "import yaml\n",
    "\n",
    "with open(\"../datasets/data/data.yaml\", \"r\") as f:\n",
    "    dataset = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c67bb7d",
   "metadata": {},
   "source": [
    "## 1. Understanding the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def explain_dataset(image_dir, label_dir, class_names, num_classes):\n",
    "    \n",
    "    # Storage variables\n",
    "    image_count = 0\n",
    "    label_count = 0\n",
    "    image_not_count = 0\n",
    "    total_bboxes = 0\n",
    "\n",
    "    images_per_class = defaultdict(int)\n",
    "    instances_per_class = defaultdict(int)\n",
    "    objects_per_image = defaultdict(int)\n",
    "\n",
    "    missing_label_files = []\n",
    "    empty_label_files = []\n",
    "    images_without_annotations = []\n",
    "    bbox_areas = []\n",
    "\n",
    "    \n",
    "    for img_file in os.listdir(image_dir):\n",
    "        if not img_file.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "            image_not_count += 1\n",
    "            continue\n",
    "        \n",
    "        image_count += 1\n",
    "        image_name = os.path.splitext(img_file)[0]\n",
    "        label_file = os.path.join(label_dir, f\"{image_name}.txt\")\n",
    "        \n",
    "        # Check if label file exists\n",
    "        if not os.path.exists(label_file):\n",
    "            missing_label_files.append(img_file)\n",
    "            images_without_annotations.append(img_file)\n",
    "            continue\n",
    "        else:\n",
    "            label_count += 1\n",
    "        \n",
    "        # Read annotate \n",
    "        with open(label_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        if lines == [''] or len(lines) == 0:\n",
    "            empty_label_files.append(img_file)\n",
    "            images_without_annotations.append(img_file)\n",
    "            continue\n",
    "        \n",
    "        # Count objects\n",
    "        objects_per_image[img_file] = len(lines)\n",
    "        total_bboxes += len(lines)\n",
    "        \n",
    "        # Count instances\n",
    "        for line in lines:\n",
    "            cls, x, y, w, h = map(float, line.split())\n",
    "            cls = int(cls)\n",
    "            \n",
    "            instances_per_class[cls] += 1\n",
    "            images_per_class[cls] += 1\n",
    "            \n",
    "            # Calculate bbox area\n",
    "            bbox_area = w * h\n",
    "            bbox_areas.append(bbox_area)\n",
    "        \n",
    "    # DISPLAY STATISTICS\n",
    "    print(\"\\n====== DATASET STATISTICS ======\\n\")\n",
    "    print(\"Total images:\", image_count)\n",
    "    print(\"Total label files:\", label_count)\n",
    "    print(\"Images other formats:\", image_not_count)\n",
    "    print(\"Total bounding boxes:\", total_bboxes)\n",
    "    print(\"Average bounding-box size (normalized area):\", \n",
    "        np.mean(bbox_areas) if bbox_areas else 0)\n",
    "\n",
    "    print(\"\\n=== IMAGES PER CLASS ===\")\n",
    "    for cls in range(num_classes):\n",
    "        print(f\"{cls} ({class_names[cls]}): {images_per_class[cls]} images\")\n",
    "\n",
    "    print(\"\\n=== INSTANCES PER CLASS ===\")\n",
    "    for cls in range(num_classes):\n",
    "        print(f\"{cls} ({class_names[cls]}): {instances_per_class[cls]} instances\")\n",
    "    \n",
    "    print(\"\\n=== OBJECTS PER IMAGE ===\")\n",
    "    avg_objects_per_img = sum(objects_per_image.values()) / len(objects_per_image)\n",
    "    print(f\"Average objects per image: {avg_objects_per_img}\")\n",
    "\n",
    "    print(\"\\nImages without annotation labels:\", len(images_without_annotations))\n",
    "    print(\"Missing label files:\", len(missing_label_files))\n",
    "    print(\"Empty label files:\", len(empty_label_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316acd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = dataset[\"names\"]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "explain_dataset(train_image_dir, train_label_dir, class_names, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd20280",
   "metadata": {},
   "source": [
    "## Train Dataset Observations:\n",
    "\n",
    "- I'm using YOLO format to understand and analyze the dataset.\n",
    "- The dataset contains two splits: train and valid.\n",
    "- Each split contains two folders:\n",
    "    - images/ → original images\n",
    "    - labels/ → YOLO annotation files\n",
    "- Each annotation file contains lines in the format:\n",
    "- class x_center y_center width height, all values normalized.\n",
    "\n",
    "## Understanding Images and Annotations\n",
    "\n",
    "- Each image may contain one or more annotations, but in this dataset most images contain exactly one object.\n",
    "- Each annotation is linked to only one image through the same filename.\n",
    "- Class IDs range from 0 to 12, mapped to 13 categories like \"banana\", \"chilli\", \"drinkcan\", etc.\n",
    "- The dataset appears to have one instance per image (based on total images = 919 and total instances = 1180, with very few multi-object images).\n",
    "\n",
    "## Parsed and Computed Statistics\n",
    "\n",
    "- Loaded all images and their corresponding YOLO annotation files.\n",
    "\n",
    "- Counted:\n",
    "    - Total number of images\n",
    "    - Total number of labels (same count → perfectly consistent)\n",
    "    - Total number of bounding boxes (1180)\n",
    "    - Bounding-box sizes\n",
    "    - Objects per image\n",
    "    - Instances per class\n",
    "    - Images per class\n",
    "- Verified data consistency:\n",
    "    - No missing labels\n",
    "    - No empty label files\n",
    "    - No images without annotations\n",
    "    - No images are any other format\n",
    "- Bounding box statistics:\n",
    "    - Average normalized bbox area = 0.3129\n",
    "    (meaning boxes are generally large relative to image size)\n",
    "\n",
    "## Class Distribution\n",
    "\n",
    "- Extracted category IDs and counted occurrences of each class.\n",
    "- The dataset has 13 object categories:\n",
    "    - The largest class is sweetpotato (120 instances).\n",
    "    - The smallest class is chilli (71 instances).\n",
    "- Class distribution is relatively balanced, with no extremely rare classes.\n",
    "\n",
    "## Counting Objects Per Image\n",
    "\n",
    "- Computed number of objects per image across both splits.\n",
    "- Most images contain a single object, since total instances (1180) is only slightly higher than total images (919).\n",
    "- The average number of objects per image ≈ 1.28.\n",
    "\n",
    "## Conclusions:\n",
    "\n",
    "- The dataset contains two YOLO-format splits: train and valid.\n",
    "- Directory structure is consistent, containing:\n",
    "    - images/\n",
    "    - labels/\n",
    "    - data.yaml defining the 13 class names.\n",
    "- All images have corresponding label files:\n",
    "    - 0 missing labels\n",
    "    - 0 empty labels\n",
    "    - 0 unannotated images\n",
    "    - 0 image are other format\n",
    "    - This indicates a clean and well-prepared dataset.\n",
    "- Each annotation follows YOLO format with class IDs and normalized bounding boxes.\n",
    "- The dataset contains:\n",
    "    - 919 images\n",
    "    - 919 label files\n",
    "    - 1180 object instances\n",
    "- Class distribution is balanced overall:\n",
    "    - Highest: sweetpotato (120)\n",
    "    - Lowest: chilli (71)\n",
    "- Bounding boxes are generally large, with an average normalized area of ~0.31.\n",
    "- On average, each image contains 1.28 objects, meaning the dataset is primarily single-object per image.\n",
    "- Category names match the 13 waste/food-related classes:\n",
    "banana, chilli, drinkcan, drinkpack, foodcan, lettuce, paperbag, plasticbag, plasticbottle, plasticcontainer, sweetpotato, teabag, tissueroll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_dataset(valid_image_dir, valid_label_dir, class_names, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f6779f",
   "metadata": {},
   "source": [
    "## Valid Dataset Observations:\n",
    "\n",
    "- I'm using YOLO format to understand and analyze the dataset.\n",
    "- The dataset contains two splits: train and valid.\n",
    "- Each split contains two folders:\n",
    "    - images/ → original images\n",
    "    - labels/ → YOLO annotation files\n",
    "- Each annotation file contains lines in the format:\n",
    "- class x_center y_center width height, all values normalized.\n",
    "\n",
    "## Understanding Images and Annotations\n",
    "\n",
    "- Each image may contain one or more annotations, but in this dataset most images contain exactly one object.\n",
    "- Each annotation is linked to only one image through the same filename.\n",
    "- Class IDs range from 0 to 12, mapped to 13 categories like \"banana\", \"chilli\", \"drinkcan\", etc.\n",
    "- The dataset appears to have one instance per image (based on total images = 459 and total instances = 702, with very few multi-object images).\n",
    "\n",
    "## Parsed and Computed Statistics\n",
    "\n",
    "- Loaded all images and their corresponding YOLO annotation files.\n",
    "\n",
    "- Counted:\n",
    "    - Total number of images\n",
    "    - Total number of labels (same count → perfectly consistent)\n",
    "    - Total number of bounding boxes (702)\n",
    "    - Bounding-box sizes\n",
    "    - Objects per image\n",
    "    - Instances per class\n",
    "    - Images per class\n",
    "- Verified data consistency:\n",
    "    - No missing labels\n",
    "    - No empty label files\n",
    "    - No images without annotations\n",
    "    - No images are any other format\n",
    "- Bounding box statistics:\n",
    "    - Average normalized bbox area = 0.2643\n",
    "    (meaning boxes are generally large relative to image size)\n",
    "\n",
    "## Class Distribution\n",
    "\n",
    "- Extracted category IDs and counted occurrences of each class.\n",
    "- The dataset has 13 object categories:\n",
    "    - The largest class is tissueroll (63 instances).\n",
    "    - The smallest class is plasticbag (40 instances).\n",
    "- Class distribution is relatively balanced, with no extremely rare classes.\n",
    "\n",
    "## Counting Objects Per Image\n",
    "\n",
    "- Computed number of objects per image across both splits.\n",
    "- Most images contain a single object, since total instances (702) is only slightly higher than total images (459).\n",
    "- The average number of objects per image ≈ 1.52.\n",
    "\n",
    "## Conclusions:\n",
    "\n",
    "- The dataset contains two YOLO-format splits: train and valid.\n",
    "- Directory structure is consistent, containing:\n",
    "    - images/\n",
    "    - labels/\n",
    "    - data.yaml defining the 13 class names.\n",
    "- All images have corresponding label files:\n",
    "    - 0 missing labels\n",
    "    - 0 empty labels\n",
    "    - 0 unannotated images\n",
    "    - 0 image are other format\n",
    "    - This indicates a clean and well-prepared dataset.\n",
    "- Each annotation follows YOLO format with class IDs and normalized bounding boxes.\n",
    "- The dataset contains:\n",
    "    - 459 images\n",
    "    - 459 label files\n",
    "    - 702 object instances\n",
    "- Class distribution is balanced overall:\n",
    "    - Highest: tissueroll (63)\n",
    "    - Lowest: plasticbag (40)\n",
    "- Bounding boxes are generally large, with an average normalized area of ~0.31.\n",
    "- On average, each image contains 1.28 objects, meaning the dataset is primarily single-object per image.\n",
    "- Category names match the 13 waste/food-related classes:\n",
    "banana, chilli, drinkcan, drinkpack, foodcan, lettuce, paperbag, plasticbag, plasticbottle, plasticcontainer, sweetpotato, teabag, tissueroll."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffe1cff",
   "metadata": {},
   "source": [
    "### 1.1 Comperehend the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94316ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = dataset[\"names\"]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "explain_dataset(train_image_dir, train_label_dir, class_names, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b3b1e5",
   "metadata": {},
   "source": [
    "## Train Dataset Observations:\n",
    "\n",
    "- I'm using YOLO format to understand and analyze the dataset.\n",
    "- The dataset contains three splits: train, valid, and test.\n",
    "- Each split contains two folders:\n",
    "    - images/ → original images\n",
    "    - labels/ → YOLO annotation files\n",
    "- Each annotation file contains lines in the format:\n",
    "- class x_center y_center width height, all values normalized.\n",
    "\n",
    "## Understanding Images and Annotations\n",
    "\n",
    "- Each image may contain one or more annotations, but in this dataset most images contain exactly one object.\n",
    "- Each annotation is linked to only one image through the same filename.\n",
    "- Class IDs range from 0 to 12, mapped to 13 categories like \"banana\", \"chilli\", \"drinkcan\", etc.\n",
    "- The dataset appears to have one instance per image (based on total images = 964 and total instances = 1235, with very few multi-object images).\n",
    "\n",
    "## Parsed and Computed Statistics\n",
    "\n",
    "- Loaded all images and their corresponding YOLO annotation files.\n",
    "\n",
    "- Counted:\n",
    "    - Total number of images\n",
    "    - Total number of labels (same count → perfectly consistent)\n",
    "    - Total number of bounding boxes (1235)\n",
    "    - Bounding-box sizes\n",
    "    - Objects per image\n",
    "    - Instances per class\n",
    "    - Images per class\n",
    "- Verified data consistency:\n",
    "    - No missing labels\n",
    "    - No empty label files\n",
    "    - No images without annotations\n",
    "    - No images are any other format\n",
    "- Bounding box statistics:\n",
    "    - Average normalized bbox area = 0.3129\n",
    "    (meaning boxes are generally large relative to image size)\n",
    "\n",
    "## Class Distribution\n",
    "\n",
    "- Extracted category IDs and counted occurrences of each class.\n",
    "- The dataset has 13 object categories:\n",
    "    - The largest class is sweetpotato (108 instances).\n",
    "    - The smallest class is banana (78 instances).\n",
    "- Class distribution is relatively balanced, with no extremely rare classes.\n",
    "\n",
    "## Counting Objects Per Image\n",
    "\n",
    "- Computed number of objects per image across both splits.\n",
    "- Most images contain a single object, since total instances (1235) is only slightly higher than total images (964).\n",
    "- The average number of objects per image ≈ 1.28.\n",
    "\n",
    "## Conclusions:\n",
    "\n",
    "- The dataset contains two YOLO-format splits: train and valid.\n",
    "- Directory structure is consistent, containing:\n",
    "    - images/\n",
    "    - labels/\n",
    "    - data.yaml defining the 13 class names.\n",
    "- All images have corresponding label files:\n",
    "    - 0 missing labels\n",
    "    - 0 empty labels\n",
    "    - 0 unannotated images\n",
    "    - 0 image are other format\n",
    "    - This indicates a clean and well-prepared dataset.\n",
    "- Each annotation follows YOLO format with class IDs and normalized bounding boxes.\n",
    "- The dataset contains:\n",
    "    - 964 images\n",
    "    - 964 label files\n",
    "    - 1235 object instances\n",
    "- Class distribution is balanced overall:\n",
    "    - Highest: sweetpotato (108)\n",
    "    - Lowest: banana (78)\n",
    "- Bounding boxes are generally large, with an average normalized area of ~0.31.\n",
    "- On average, each image contains 1.28 objects, meaning the dataset is primarily single-object per image.\n",
    "- Category names match the 13 waste/food-related classes:\n",
    "banana, chilli, drinkcan, drinkpack, foodcan, lettuce, paperbag, plasticbag, plasticbottle, plasticcontainer, sweetpotato, teabag, tissueroll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a8542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = dataset[\"names\"]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "explain_dataset(valid_image_dir, valid_label_dir, class_names, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae83b86",
   "metadata": {},
   "source": [
    "## Valid Dataset Observations:\n",
    "\n",
    "- I'm using YOLO format to understand and analyze the dataset.\n",
    "- The dataset contains three splits: train, valid, and test.\n",
    "- Each split contains two folders:\n",
    "    - images/ → original images\n",
    "    - labels/ → YOLO annotation files\n",
    "- Each annotation file contains lines in the format:\n",
    "- class x_center y_center width height, all values normalized.\n",
    "\n",
    "## Understanding Images and Annotations\n",
    "\n",
    "- Each image may contain one or more annotations, but in this dataset most images contain exactly one object.\n",
    "- Each annotation is linked to only one image through the same filename.\n",
    "- Class IDs range from 0 to 12, mapped to 13 categories like \"banana\", \"chilli\", \"drinkcan\", etc.\n",
    "- The dataset appears to have one instance per image (based on total images = 275 and total instances = 408, with very few multi-object images).\n",
    "\n",
    "## Parsed and Computed Statistics\n",
    "\n",
    "- Loaded all images and their corresponding YOLO annotation files.\n",
    "\n",
    "- Counted:\n",
    "    - Total number of images\n",
    "    - Total number of labels (same count → perfectly consistent)\n",
    "    - Total number of bounding boxes (408)\n",
    "    - Bounding-box sizes\n",
    "    - Objects per image\n",
    "    - Instances per class\n",
    "    - Images per class\n",
    "- Verified data consistency:\n",
    "    - No missing labels\n",
    "    - No empty label files\n",
    "    - No images without annotations\n",
    "    - No images are any other format\n",
    "- Bounding box statistics:\n",
    "    - Average normalized bbox area = 0.2745\n",
    "    (meaning boxes are generally large relative to image size)\n",
    "\n",
    "## Class Distribution\n",
    "\n",
    "- Extracted category IDs and counted occurrences of each class.\n",
    "- The dataset has 13 object categories:\n",
    "    - The largest class is sweetpotato (52 instances).\n",
    "    - The smallest class is chilli (19 instances).\n",
    "- Class distribution is relatively balanced, with no extremely rare classes.\n",
    "\n",
    "## Counting Objects Per Image\n",
    "\n",
    "- Computed number of objects per image across both splits.\n",
    "- Most images contain a single object, since total instances (408) is only slightly higher than total images (275).\n",
    "- The average number of objects per image ≈ 1.48.\n",
    "\n",
    "## Conclusions:\n",
    "\n",
    "- The dataset contains two YOLO-format splits: train and valid.\n",
    "- Directory structure is consistent, containing:\n",
    "    - images/\n",
    "    - labels/\n",
    "    - data.yaml defining the 13 class names.\n",
    "- All images have corresponding label files:\n",
    "    - 0 missing labels\n",
    "    - 0 empty labels\n",
    "    - 0 unannotated images\n",
    "    - 0 image are other format\n",
    "    - This indicates a clean and well-prepared dataset.\n",
    "- Each annotation follows YOLO format with class IDs and normalized bounding boxes.\n",
    "- The dataset contains:\n",
    "    - 275 images\n",
    "    - 275 label files\n",
    "    - 408 object instances\n",
    "- Class distribution is balanced overall:\n",
    "    - Highest: sweetpotato (52)\n",
    "    - Lowest: chilli (19)\n",
    "- Bounding boxes are generally large, with an average normalized area of ~0.2745.\n",
    "- On average, each image contains 1.48 objects, meaning the dataset is primarily single-object per image.\n",
    "- Category names match the 13 waste/food-related classes:\n",
    "banana, chilli, drinkcan, drinkpack, foodcan, lettuce, paperbag, plasticbag, plasticbottle, plasticcontainer, sweetpotato, teabag, tissueroll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232402d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = dataset[\"names\"]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "explain_dataset(test_image_dir, test_label_dir, class_names, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aeabbb",
   "metadata": {},
   "source": [
    "## Test Dataset Observations:\n",
    "\n",
    "- I'm using YOLO format to understand and analyze the dataset.\n",
    "- The dataset contains three splits: train, valid, and test.\n",
    "- Each split contains two folders:\n",
    "    - images/ → original images\n",
    "    - labels/ → YOLO annotation files\n",
    "- Each annotation file contains lines in the format:\n",
    "- class x_center y_center width height, all values normalized.\n",
    "\n",
    "## Understanding Images and Annotations\n",
    "\n",
    "- Each image may contain one or more annotations, but in this dataset most images contain exactly one object.\n",
    "- Each annotation is linked to only one image through the same filename.\n",
    "- Class IDs range from 0 to 12, mapped to 13 categories like \"banana\", \"chilli\", \"drinkcan\", etc.\n",
    "- The dataset appears to have one instance per image (based on total images = 139 and total instances = 239, with very few multi-object images).\n",
    "\n",
    "## Parsed and Computed Statistics\n",
    "\n",
    "- Loaded all images and their corresponding YOLO annotation files.\n",
    "\n",
    "- Counted:\n",
    "    - Total number of images\n",
    "    - Total number of labels (same count → perfectly consistent)\n",
    "    - Total number of bounding boxes (239)\n",
    "    - Bounding-box sizes\n",
    "    - Objects per image\n",
    "    - Instances per class\n",
    "    - Images per class\n",
    "- Verified data consistency:\n",
    "    - No missing labels\n",
    "    - No empty label files\n",
    "    - No images without annotations\n",
    "    - No images are any other format\n",
    "- Bounding box statistics:\n",
    "    - Average normalized bbox area = 0.2353\n",
    "    (meaning boxes are generally large relative to image size)\n",
    "\n",
    "## Class Distribution\n",
    "\n",
    "- Extracted category IDs and counted occurrences of each class.\n",
    "- The dataset has 13 object categories:\n",
    "    - The largest class is sweetpotato (31 instances).\n",
    "    - The smallest class is chilli, drinkcan, foodcan (15 instances).\n",
    "- Class distribution is relatively balanced, with no extremely rare classes.\n",
    "\n",
    "## Counting Objects Per Image\n",
    "\n",
    "- Computed number of objects per image across both splits.\n",
    "- Most images contain a single object, since total instances (239) is only slightly higher than total images (139).\n",
    "- The average number of objects per image ≈ 1.7194\n",
    "\n",
    "## Conclusions:\n",
    "\n",
    "- The dataset contains two YOLO-format splits: train and valid.\n",
    "- Directory structure is consistent, containing:\n",
    "    - images/\n",
    "    - labels/\n",
    "    - data.yaml defining the 13 class names.\n",
    "- All images have corresponding label files:\n",
    "    - 0 missing labels\n",
    "    - 0 empty labels\n",
    "    - 0 unannotated images\n",
    "    - 0 image are other format\n",
    "    - This indicates a clean and well-prepared dataset.\n",
    "- Each annotation follows YOLO format with class IDs and normalized bounding boxes.\n",
    "- The dataset contains:\n",
    "    - 139 images\n",
    "    - 139 label files\n",
    "    - 239 object instances\n",
    "- Class distribution is balanced overall:\n",
    "    - Highest: sweetpotato (31)\n",
    "    - Lowest: chilli, foodcan, drinkcan (15)\n",
    "- Bounding boxes are generally large, with an average normalized area of ~0.2353.\n",
    "- On average, each image contains 1.7194 objects, meaning the dataset is primarily single-object per image.\n",
    "- Category names match the 13 waste/food-related classes:\n",
    "banana, chilli, drinkcan, drinkpack, foodcan, lettuce, paperbag, plasticbag, plasticbottle, plasticcontainer, sweetpotato, teabag, tissueroll."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aefe37",
   "metadata": {},
   "source": [
    "## 2. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f90cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_class_distribution_and_percentage_analysis(class_names, instances_per_class, num_classes):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.barplot(x=list(class_names), y=[instances_per_class[i] for i in range(num_classes)], palette=\"viridis\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(\"Instances per Class\")\n",
    "    plt.show()\n",
    "\n",
    "    # Percentage\n",
    "    total_objects = sum(instances_per_class.values())\n",
    "    percentages = {class_names[i]: (instances_per_class[i]/total_objects*100) for i in range(num_classes)}\n",
    "    categoies = list(percentages.keys())\n",
    "    values = list(percentages.values())\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.pie(values, labels=categoies, autopct=\"%1.2f%%\",  startangle=90, shadow=True, labeldistance=1.1, pctdistance=0.6)\n",
    "    plt.title(\"Class Percentage Distribution\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_distribution_analysis(bbox_areas, bbox_widths, bbox_heights, bbox_centers):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.histplot(bbox_areas, bins=20, kde=True)\n",
    "    plt.title(\"Bounding Box Area Distribution (normalized)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Width distribution\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.histplot(bbox_widths, bins=20, kde=True)\n",
    "    plt.title(\"Bounding Box Width Distribution\")\n",
    "    plt.show()\n",
    "\n",
    "    # Height distribution\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.histplot(bbox_heights, bins=20, kde=True)\n",
    "    plt.title(\"Bounding Box Height Distribution\")\n",
    "    plt.show()\n",
    "\n",
    "    # Heatmap of bbox centers\n",
    "    xs = [c[0] for c in bbox_centers]\n",
    "    ys = [c[1] for c in bbox_centers]\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.kdeplot(x=xs, y=ys, fill=True, cmap=\"Reds\")\n",
    "    plt.title(\"Bounding Box Center Heatmap\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c836d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_resolution_analysis(image_info):\n",
    "    widths = [w for _, w, _ in image_info]\n",
    "    heights = [h for _, _, h in image_info]\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(widths, heights, alpha=0.4)\n",
    "    plt.xlabel(\"Width\")\n",
    "    plt.ylabel(\"Height\")\n",
    "    plt.title(\"Image Resolution Distribution\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(widths, bins=20, density=True)\n",
    "    plt.xlabel(\"Width\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(\"Width Distribution\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(heights, bins=20, density=True)\n",
    "    plt.xlabel(\"Height\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(\"Height Distribution\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Average resolution:\", (np.mean(widths), np.mean(heights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30e2b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bounding box area size distribution\n",
    "def bbx_size(bbox_areas, aspect_ratios):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(bbox_areas, bins=20, kde=True)\n",
    "    plt.title('Bounding Box Area Distribution')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(aspect_ratios, bins=20, kde=True)\n",
    "    plt.title('Bounding Box Aspect Ratio Distribution')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Bounding box area distribution: {np.percentile(bbox_areas, [25, 50, 75])}\")\n",
    "    print(f\"Aspect ratio distribution: {np.percentile(aspect_ratios, [25, 50, 75])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa6fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_analysis(bbox_areas):\n",
    "    areas = np.array(bbox_areas)\n",
    "    mean_area = np.mean(areas)\n",
    "    std_area = np.std(areas)\n",
    "    outliers = np.where((areas > mean_area + 3*std_area) | (areas < mean_area - 3*std_area))[0]\n",
    "\n",
    "    print(\"\\nBounding box outliers:\", len(outliers))\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.boxplot(x=areas)\n",
    "    plt.title(\"Boxplot of Normalized BBox Area\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733ea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurrence_analysis(co_occurrence, class_names, num_classes):\n",
    "    matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "    for c1 in co_occurrence:\n",
    "        for c2 in co_occurrence[c1]:\n",
    "            matrix[int(c1)][int(c2)] = co_occurrence[c1][c2]\n",
    "\n",
    "    plt.figure(figsize=(12,10))\n",
    "    sns.heatmap(matrix, xticklabels=class_names, yticklabels=class_names, cmap=\"Blues\", annot=True)\n",
    "    plt.title(\"Class Co-occurrence Heatmap\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b6744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_samples(annotations, n=3):\n",
    "    samples = random.sample(annotations, n)\n",
    "    \n",
    "    for img_path, cls, xc, yc, bw, bh in samples:\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"Failed to load: {img_path}\")\n",
    "            continue\n",
    "        \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        print(f\"Image format: {img.dtype}, Image size: {img.size}, Image shape: {img.shape}\")\n",
    "        \n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Numeric values\n",
    "        xc, yc, bw, bh = map(float, (xc, yc, bw, bh))\n",
    "\n",
    "        x1 = (xc - bw/2) * w\n",
    "        y1 = (yc - bh/2) * h\n",
    "        width = bw * w\n",
    "        height = bh * h\n",
    "        \n",
    "        # clamp to image bounds\n",
    "        x1 = max(0, x1)\n",
    "        y1 = max(0, y1)\n",
    "        x2 = min(w, x1 + width)\n",
    "        y2 = min(h, y1 + height)\n",
    "        \n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        \n",
    "        # Draw everyhing on the same figure\n",
    "        plt.figure(figsize=(6,6))\n",
    "        ax = plt.gca()\n",
    "        plt.imshow(img)\n",
    "        \n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), width, height, \n",
    "            linewidth=2, edgecolor='r', facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        plt.text(x1, max(0, y1-5), class_names[int(cls)], \n",
    "                 fontsize=12, color='r', bbox=dict(facecolor='yellow', alpha=0.5)\n",
    "        )\n",
    "        \n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355c9ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_dataset(image_dir, label_dir, class_names, num_classes):\n",
    "    \n",
    "    # Storage variables\n",
    "    image_info = []\n",
    "    annotations = []\n",
    "    image_not_count = 0\n",
    "\n",
    "    objects_per_image = Counter()\n",
    "    instances_per_class = Counter()\n",
    "    co_occurrence = defaultdict(lambda: Counter())\n",
    "\n",
    "    missing_labels = []\n",
    "    empty_annotations = []\n",
    "    bbox_areas = []\n",
    "    aspect_ratios = []\n",
    "    bbox_widths = []\n",
    "    bbox_heights = []\n",
    "    bbox_centers = []\n",
    "    \n",
    "    for img_file in tqdm(os.listdir(image_dir)):\n",
    "        if not img_file.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "            image_not_count += 1\n",
    "            continue\n",
    "        \n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        label_path = os.path.join(label_dir, img_file.rsplit(\".\",1)[0] + \".txt\")\n",
    "        \n",
    "        # Load image resolution\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        if img is None:\n",
    "            continue\n",
    "        \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_h, img_w = img.shape[:2]\n",
    "        image_info.append((img_path, img_w, img_h))\n",
    "        \n",
    "        # Check annotations\n",
    "        if not os.path.exists(label_path):\n",
    "            missing_labels.append(img_path)\n",
    "            continue\n",
    "        \n",
    "        with open(label_path, \"r\") as f:\n",
    "            lines = [l.strip() for l in f.readlines() if l.strip()]\n",
    "        \n",
    "        if len(lines) == 0:\n",
    "            empty_annotations.append(img_path)\n",
    "            continue\n",
    "        \n",
    "        objects_per_image[img_path] = len(lines)\n",
    "        \n",
    "        # Track which classes in this image\n",
    "        present_classes = set()\n",
    "        \n",
    "        for line in lines:\n",
    "            cls, xc, yc, bw, bh = map(float, line.split())\n",
    "            \n",
    "            bbox_widths.append(bw)\n",
    "            bbox_heights.append(bh)\n",
    "            bbox_centers.append((xc, yc))\n",
    "            \n",
    "            bbox_area = bw * bh\n",
    "            bbox_areas.append(bbox_area)\n",
    "            \n",
    "            aspect_ratio = bw / bh\n",
    "            aspect_ratios.append(aspect_ratio)\n",
    "            \n",
    "            annotations.append((img_path, cls, xc, yc, bw, bh))\n",
    "            present_classes.add(cls)\n",
    "            instances_per_class[cls] += 1\n",
    "            \n",
    "            for c in present_classes:\n",
    "                co_occurrence[int(cls)][int(c)] += 1\n",
    "        \n",
    "        for c1 in present_classes:\n",
    "            for c2 in present_classes:\n",
    "                if c1 != c2:\n",
    "                    co_occurrence[c1][c2] += 1\n",
    "    \n",
    "    print(\"\\n=== MISSING LABELS ===\")\n",
    "    print(\"Missing labels:\", len(missing_labels))\n",
    "    \n",
    "    print(\"\\n=== EMPTY ANNOTATIONS ===\")\n",
    "    print(\"Empty annotations:\", len(empty_annotations))\n",
    "    \n",
    "    print(\"\\n=== CLASS DISTRIBUTION ===\")\n",
    "    object_class_distribution_and_percentage_analysis(class_names, instances_per_class, num_classes)\n",
    "    \n",
    "    print(\"\\n=== BOUNDING BOX ANALYSIS ===\")\n",
    "    area_distribution_analysis(bbox_areas, bbox_widths, bbox_heights, bbox_centers)\n",
    "    \n",
    "    print(\"\\n=== IMAGE RESOLUTION ANALYSIS ===\")\n",
    "    image_resolution_analysis(image_info)\n",
    "    \n",
    "    print(\"\\n=== AREA SIZE ===\")\n",
    "    bbx_size(bbox_areas, aspect_ratios)\n",
    "    \n",
    "    print(\"\\n=== OUTLIERS ===\")\n",
    "    outlier_analysis(bbox_areas)\n",
    "    \n",
    "    print(\"\\n=== CO-OCCURRENCE ===\")\n",
    "    co_occurrence_analysis(co_occurrence, class_names, num_classes)\n",
    "    \n",
    "    print(\"\\n=== RANDOM SAMPLES ===\")\n",
    "    show_random_samples(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid dataset\n",
    "class_names = dataset[\"names\"]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "parse_dataset(valid_image_dir, valid_label_dir, class_names, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3640b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting objects per image\n",
    "def img_count(image_dir, label_dir):\n",
    "    objects_per_image = defaultdict(int)\n",
    "    \n",
    "    for img_file in os.listdir(image_dir):\n",
    "        image_name = os.path.splitext(img_file)[0]\n",
    "        label_file = os.path.join(label_dir, f\"{image_name}.txt\")\n",
    "                \n",
    "        # Read annotate \n",
    "        with open(label_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        if lines == [''] or len(lines) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Count objects\n",
    "        objects_per_image[img_file] = len(lines)\n",
    "        \n",
    "    return objects_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a4eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(objects_per_count):\n",
    "    sns.histplot(objects_per_count, kde=True)\n",
    "    plt.title('Objects per Image Distribution')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_count_distribution(image_dir, label_dir):\n",
    "    # Dataset object per image count\n",
    "    data_object_count = img_count(image_dir, label_dir)\n",
    "    print(f\"Object per image: {Counter(data_object_count.values())}\")\n",
    "    object_count_values = list(data_object_count.values())\n",
    "\n",
    "    plot_histogram(object_count_values)\n",
    "\n",
    "    return data_object_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid dataset object per image count\n",
    "\n",
    "valid_object_count = object_count_distribution(valid_image_dir, valid_label_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_with_multiple_objects(object_count, category_id):\n",
    "    data_multiple_objects = [image_id for image_id, count in object_count.items() if count == category_id]\n",
    "\n",
    "    print(f\"Images with {category_id} objects: {data_multiple_objects}\")\n",
    "    print(f\"Images total lengths with multiple objects: {len(data_multiple_objects)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid dataset Identify images with 2 or more objects\n",
    "images_with_multiple_objects(valid_object_count, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bc70d3",
   "metadata": {},
   "source": [
    "## Valid Dataset Observations:\n",
    "\n",
    "- Plot a graph of the 'Number of Objects' and 'Categories'. I've many categories of: 'Waste Material'.\n",
    "    - The validation dataset contains 13 different object categories.\n",
    "    - The distribution of objects across categories shows class imbalance, with some categories appearing more frequently than others.\n",
    "- Plot a bounding box width and height distributions.\n",
    "    - Bounding box width and height distributions indicate that most objects occupy a moderate-to-large portion of the image.\n",
    "    - The histograms show the normalized bounding box width and height values.\n",
    "- Plot an Image size distribution graph\n",
    "    - All validation images have a fixed resolution of 640 × 480 pixels.\n",
    "    - No variation in image size is observed across the validation dataset.\n",
    "- Plot a graph of bounding box annotation visualization\n",
    "    - We know about the images format, size, and shape: Image format: uint8, Image size: 921600, Image shape: (640, 480, 3)\n",
    "    - Visualized samples confirm that bounding boxes are correctly aligned with objects.\n",
    "    - Some images contain multiple bounding boxes, indicating the presence of more than one object per image.\n",
    "- Plot a graph of bounding box area size and aspect ratio distribution\n",
    "    - Bounding box area distribution shows that most objects have relatively large bounding box areas compared to the image size.\n",
    "    - Aspect ratio distribution indicates that most bounding boxes are close to square.\n",
    "    - The histograms of bounding box area and aspect ratio values are right-skewed.\n",
    "- Plot a bounding box area size and aspect ratio distribution\n",
    "    - Bounding box area distribution shows that most objects have relatively large bounding box areas compared to the image size.\n",
    "    - Aspect ratio distribution indicates that most bounding boxes are close to square.\n",
    "    - The histograms of bounding box area and aspect ratio values are right-skewed.\n",
    "- Plot a graph of object per image count\n",
    "    - The validation dataset contains both single-object and multi-object images.\n",
    "    - Object-per-image count for the validation dataset: Object per image: Counter({1: 427, 12: 12, 2: 10, 11: 8, 13: 1, 10: 1})\n",
    "\n",
    "\n",
    "## Conclusions:\n",
    "\n",
    "- The validation dataset contains 13 object categories, covering various waste and food-related classes.\n",
    "- The number of objects per category is imbalanced, which may influence validation performance across different classes.\n",
    "- Bounding box width, height, and area distributions indicate that objects are generally moderate to large in size, simplifying object localization compared to datasets dominated by small objects.\n",
    "- All images have a uniform resolution of 640 × 480 pixels, which ensures consistent input dimensions during model validation.\n",
    "- Bounding box annotation visualizations confirm that annotations are accurate and properly aligned with the objects in the images.\n",
    "- Bounding box area and aspect ratio distributions show that most objects are square-shaped, with few extreme aspect ratios.\n",
    "- Object-per-image analysis shows that many validation images contain multiple object instances, increasing scene complexity.\n",
    "- The presence of multiple objects and class imbalance in the validation dataset may impact validation metrics, particularly for less frequent categories.\n",
    "- Overall, the validation dataset is clean, well-annotated, and suitable for evaluating YOLO-based object detection models, while requiring careful interpretation of results due to class imbalance and varying object counts per image.\n",
    "- Category names match the 13 waste/food-related classes:\n",
    "banana, chilli, drinkcan, drinkpack, foodcan, lettuce, paperbag, plasticbag, plasticbottle, plasticcontainer, sweetpotato, teabag, tissueroll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad3cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "class_names = dataset[\"names\"]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "parse_dataset(train_image_dir, train_label_dir, class_names, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset object per image count\n",
    "\n",
    "train_object_count = object_count_distribution(train_image_dir, train_label_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid dataset Identify images with 2 or more objects\n",
    "images_with_multiple_objects(train_object_count, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7225dc60",
   "metadata": {},
   "source": [
    "## Train Dataset Observations:\n",
    "\n",
    "- Plot a graph of the 'Number of Objects' and 'Categories'. I've many categories of: 'Waste Material'.\n",
    "    - The train dataset contains 13 different object categories.\n",
    "    - The distribution of objects across categories shows class imbalance, with some categories appearing more frequently than others.\n",
    "- Plot a bounding box width and height distributions.\n",
    "    - Bounding box width and height distributions indicate that most objects occupy a moderate-to-large portion of the image.\n",
    "    - The histograms show the normalized bounding box width and height values.\n",
    "- Plot an Image size distribution graph\n",
    "    - All train images have a fixed resolution of 640 × 480 pixels.\n",
    "    - No variation in image size is observed across the train dataset.\n",
    "- Plot a graph of bounding box annotation visualization\n",
    "    - We know about the images format, size, and shape: Image format: uint8, Image size: 921600, Image shape: (640, 480, 3)\n",
    "    - Visualized samples confirm that bounding boxes are correctly aligned with objects.\n",
    "    - Some images contain multiple bounding boxes, indicating the presence of more than one object per image.\n",
    "- Plot a graph of bounding box area size and aspect ratio distribution\n",
    "    - Bounding box area distribution shows that most objects have relatively large bounding box areas compared to the image size.\n",
    "    - Aspect ratio distribution indicates that most bounding boxes are close to square.\n",
    "    - The histograms of bounding box area and aspect ratio values are right-skewed.\n",
    "- Plot a bounding box area size and aspect ratio distribution\n",
    "    - Bounding box area distribution shows that most objects have relatively large bounding box areas compared to the image size.\n",
    "    - Aspect ratio distribution indicates that most bounding boxes are close to square.\n",
    "    - The histograms of bounding box area and aspect ratio values are right-skewed.\n",
    "- Plot a graph of object per image count\n",
    "    - The train dataset contains both single-object and multi-object images.\n",
    "    - Object-per-image count for the train dataset: Object per image: Counter({1: 880, 2: 16, 12: 11, 11: 7, 13: 3, 10: 2})\n",
    "\n",
    "\n",
    "## Conclusions:\n",
    "\n",
    "- The train dataset contains 13 object categories, covering various waste and food-related classes.\n",
    "- The number of objects per category is imbalanced, which may influence train performance across different classes.\n",
    "- Bounding box width, height, and area distributions indicate that objects are generally moderate to large in size, simplifying object localization compared to datasets dominated by small objects.\n",
    "- All images have a uniform resolution of 640 × 480 pixels, which ensures consistent input dimensions during model train.\n",
    "- Bounding box annotation visualizations confirm that annotations are accurate and properly aligned with the objects in the images.\n",
    "- Bounding box area and aspect ratio distributions show that most objects are square-shaped, with few extreme aspect ratios.\n",
    "- Object-per-image analysis shows that many train images contain multiple object instances, increasing scene complexity.\n",
    "- The presence of multiple objects and class imbalance in the train dataset may impact train metrics, particularly for less frequent categories.\n",
    "- Overall, the train dataset is clean, well-annotated, and suitable for evaluating YOLO-based object detection models, while requiring careful interpretation of results due to class imbalance and varying object counts per image.\n",
    "- Category names match the 13 waste/food-related classes:\n",
    "banana, chilli, drinkcan, drinkpack, foodcan, lettuce, paperbag, plasticbag, plasticbottle, plasticcontainer, sweetpotato, teabag, tissueroll."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33dc81c",
   "metadata": {},
   "source": [
    "### 2.2 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a989cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "class_names = dataset[\"names\"]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "parse_dataset(train_image_dir, train_label_dir, class_names, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e7e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset object per image count\n",
    "\n",
    "train_object_count = object_count_distribution(train_image_dir, train_label_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ac9664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset Identify images with 2 or more objects\n",
    "images_with_multiple_objects(train_object_count, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ed829",
   "metadata": {},
   "source": [
    "## Train Dataset Observations:\n",
    "\n",
    "- Plot a graph of the 'Number of Objects' and 'Categories'. I've many categories of: 'Waste Material'.\n",
    "    - The train dataset contains 13 different object categories.\n",
    "    - The distribution of objects across categories shows class imbalance, with some categories appearing more frequently than others.\n",
    "- Plot a bounding box width and height distributions.\n",
    "    - Bounding box width and height distributions indicate that most objects occupy a moderate-to-large portion of the image.\n",
    "    - The histograms show the normalized bounding box width and height values.\n",
    "- Plot an Image size distribution graph\n",
    "    - All train images have a fixed resolution of 640 × 480 pixels.\n",
    "    - No variation in image size is observed across the train dataset.\n",
    "- Plot a graph of bounding box annotation visualization\n",
    "    - We know about the images format, size, and shape: Image format: uint8, Image size: 921600, Image shape: (640, 480, 3)\n",
    "    - Visualized samples confirm that bounding boxes are correctly aligned with objects.\n",
    "    - Some images contain multiple bounding boxes, indicating the presence of more than one object per image.\n",
    "- Plot a graph of bounding box area size and aspect ratio distribution\n",
    "    - Bounding box area distribution shows that most objects have relatively large bounding box areas compared to the image size.\n",
    "    - Aspect ratio distribution indicates that most bounding boxes are close to square.\n",
    "    - The histograms of bounding box area and aspect ratio values are right-skewed.\n",
    "- Plot a bounding box area size and aspect ratio distribution\n",
    "    - Bounding box area distribution shows that most objects have relatively large bounding box areas compared to the image size.\n",
    "    - Aspect ratio distribution indicates that most bounding boxes are close to square.\n",
    "    - The histograms of bounding box area and aspect ratio values are right-skewed.\n",
    "- Plot a graph of object per image count\n",
    "    - The train dataset contains both single-object and multi-object images.\n",
    "    - Object-per-image count for the train dataset: Object per image: Counter({1: 923, 2: 17, 12: 12, 11: 8, 13: 2, 10: 2})\n",
    "\n",
    "\n",
    "## Conclusions:\n",
    "\n",
    "- The train dataset contains 13 object categories, covering various waste and food-related classes.\n",
    "- The number of objects per category is imbalanced, which may influence train performance across different classes.\n",
    "- Bounding box width, height, and area distributions indicate that objects are generally moderate to large in size, simplifying object localization compared to datasets dominated by small objects.\n",
    "- All images have a uniform resolution of 640 × 480 pixels, which ensures consistent input dimensions during model train.\n",
    "- Bounding box annotation visualizations confirm that annotations are accurate and properly aligned with the objects in the images.\n",
    "- Bounding box area and aspect ratio distributions show that most objects are square-shaped, with few extreme aspect ratios.\n",
    "- Object-per-image analysis shows that many train images contain multiple object instances, increasing scene complexity.\n",
    "- The presence of multiple objects and class imbalance in the train dataset may impact train metrics, particularly for less frequent categories.\n",
    "- Overall, the train dataset is clean, well-annotated, and suitable for evaluating YOLO-based object detection models, while requiring careful interpretation of results due to class imbalance and varying object counts per image.\n",
    "- Category names match the 13 waste/food-related classes:\n",
    "banana, chilli, drinkcan, drinkpack, foodcan, lettuce, paperbag, plasticbag, plasticbottle, plasticcontainer, sweetpotato, teabag, tissueroll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid dataset\n",
    "class_names = dataset[\"names\"]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "parse_dataset(valid_image_dir, valid_label_dir, class_names, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid dataset object per image count\n",
    "\n",
    "valid_object_count = object_count_distribution(valid_image_dir, valid_label_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid dataset Identify images with 2 or more objects\n",
    "images_with_multiple_objects(valid_object_count, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74294e70",
   "metadata": {},
   "source": [
    "## Valid Dataset Observations:\n",
    "\n",
    "- Plot a graph of the 'Number of Objects' and 'Categories'. I've many categories of: 'Waste Material'.\n",
    "    - The validation dataset contains 13 different object categories.\n",
    "    - The distribution of objects across categories shows class imbalance, with some categories appearing more frequently than others.\n",
    "- Plot a bounding box width and height distributions.\n",
    "    - Bounding box width and height distributions indicate that most objects occupy a moderate-to-large portion of the image.\n",
    "    - The histograms show the normalized bounding box width and height values.\n",
    "- Plot an Image size distribution graph\n",
    "    - All validation images have a fixed resolution of 640 × 480 pixels.\n",
    "    - No variation in image size is observed across the validation dataset.\n",
    "- Plot a graph of bounding box annotation visualization\n",
    "    - We know about the images format, size, and shape: Image format: uint8, Image size: 921600, Image shape: (640, 480, 3)\n",
    "    - Visualized samples confirm that bounding boxes are correctly aligned with objects.\n",
    "    - Some images contain multiple bounding boxes, indicating the presence of more than one object per image.\n",
    "- Plot a graph of bounding box area size and aspect ratio distribution\n",
    "    - Bounding box area distribution shows that most objects have relatively large bounding box areas compared to the image size.\n",
    "    - Aspect ratio distribution indicates that most bounding boxes are close to square.\n",
    "    - The histograms of bounding box area and aspect ratio values are right-skewed.\n",
    "- Plot a bounding box area size and aspect ratio distribution\n",
    "    - Bounding box area distribution shows that most objects have relatively large bounding box areas compared to the image size.\n",
    "    - Aspect ratio distribution indicates that most bounding boxes are close to square.\n",
    "    - The histograms of bounding box area and aspect ratio values are right-skewed.\n",
    "- Plot a graph of object per image count\n",
    "    - The validation dataset contains both single-object and multi-object images.\n",
    "    - Object-per-image count for the validation dataset: Object per image: Counter({1: 257, 2: 6, 12: 6, 11: 4, 13: 1, 10: 1})\n",
    "\n",
    "\n",
    "## Conclusions:\n",
    "\n",
    "- The validation dataset contains 13 object categories, covering various waste and food-related classes.\n",
    "- The number of objects per category is imbalanced, which may influence validation performance across different classes.\n",
    "- Bounding box width, height, and area distributions indicate that objects are generally moderate to large in size, simplifying object localization compared to datasets dominated by small objects.\n",
    "- All images have a uniform resolution of 640 × 480 pixels, which ensures consistent input dimensions during model validation.\n",
    "- Bounding box annotation visualizations confirm that annotations are accurate and properly aligned with the objects in the images.\n",
    "- Bounding box area and aspect ratio distributions show that most objects are square-shaped, with few extreme aspect ratios.\n",
    "- Object-per-image analysis shows that many validation images contain multiple object instances, increasing scene complexity.\n",
    "- The presence of multiple objects and class imbalance in the validation dataset may impact validation metrics, particularly for less frequent categories.\n",
    "- Overall, the validation dataset is clean, well-annotated, and suitable for evaluating YOLO-based object detection models, while requiring careful interpretation of results due to class imbalance and varying object counts per image.\n",
    "- Category names match the 13 waste/food-related classes:\n",
    "banana, chilli, drinkcan, drinkpack, foodcan, lettuce, paperbag, plasticbag, plasticbottle, plasticcontainer, sweetpotato, teabag, tissueroll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b70376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "class_names = dataset[\"names\"]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "parse_dataset(test_image_dir, test_label_dir, class_names, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset object per image count\n",
    "\n",
    "test_object_count = object_count_distribution(test_image_dir, test_label_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset Identify images with 2 or more objects\n",
    "images_with_multiple_objects(test_object_count, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e03032",
   "metadata": {},
   "source": [
    "## Test Dataset Observations:\n",
    "\n",
    "- Plot a graph of the 'Number of Objects' and 'Categories'. I've many categories of: 'Waste Material'.\n",
    "    - The test dataset contains 13 different object categories.\n",
    "    - The distribution of objects across categories shows class imbalance, with some categories appearing more frequently than others.\n",
    "- Plot a bounding box width and height distributions.\n",
    "    - Bounding box width and height distributions indicate that most objects occupy a moderate-to-large portion of the image.\n",
    "    - The histograms show the normalized bounding box width and height values.\n",
    "- Plot an Image size distribution graph\n",
    "    - All test images have a fixed resolution of 640 × 480 pixels.\n",
    "    - No variation in image size is observed across the test dataset.\n",
    "- Plot a graph of bounding box annotation visualization\n",
    "    - We know about the images format, size, and shape: Image format: uint8, Image size: 921600, Image shape: (640, 480, 3)\n",
    "    - Visualized samples confirm that bounding boxes are correctly aligned with objects.\n",
    "    - Some images contain multiple bounding boxes, indicating the presence of more than one object per image.\n",
    "- Plot a graph of bounding box area size and aspect ratio distribution\n",
    "    - Bounding box area distribution shows that most objects have relatively large bounding box areas compared to the image size.\n",
    "    - Aspect ratio distribution indicates that most bounding boxes are close to square.\n",
    "    - The histograms of bounding box area and aspect ratio values are right-skewed.\n",
    "- Plot a bounding box area size and aspect ratio distribution\n",
    "    - Bounding box area distribution shows that most objects have relatively large bounding box areas compared to the image size.\n",
    "    - Aspect ratio distribution indicates that most bounding boxes are close to square.\n",
    "    - The histograms of bounding box area and aspect ratio values are right-skewed.\n",
    "- Plot a graph of object per image count\n",
    "    - The test dataset contains both single-object and multi-object images.\n",
    "    - Object-per-image count for the test dataset: Object per image: Counter({1: 127, 12: 5, 2: 3, 11: 3, 13: 1})\n",
    "\n",
    "\n",
    "## Conclusions:\n",
    "\n",
    "- The test dataset contains 13 object categories, covering various waste and food-related classes.\n",
    "- The number of objects per category is imbalanced, which may influence test performance across different classes.\n",
    "- Bounding box width, height, and area distributions indicate that objects are generally moderate to large in size, simplifying object localization compared to datasets dominated by small objects.\n",
    "- All images have a uniform resolution of 640 × 480 pixels, which ensures consistent input dimensions during model test.\n",
    "- Bounding box annotation visualizations confirm that annotations are accurate and properly aligned with the objects in the images.\n",
    "- Bounding box area and aspect ratio distributions show that most objects are square-shaped, with few extreme aspect ratios.\n",
    "- Object-per-image analysis shows that many test images contain multiple object instances, increasing scene complexity.\n",
    "- The presence of multiple objects and class imbalance in the test dataset may impact test metrics, particularly for less frequent categories.\n",
    "- Overall, the test dataset is clean, well-annotated, and suitable for evaluating YOLO-based object detection models, while requiring careful interpretation of results due to class imbalance and varying object counts per image.\n",
    "- Category names match the 13 waste/food-related classes:\n",
    "banana, chilli, drinkcan, drinkpack, foodcan, lettuce, paperbag, plasticbag, plasticbottle, plasticcontainer, sweetpotato, teabag, tissueroll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18953a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dataset split\n",
    "dataset_size = [len(os.listdir(train_image_dir)), len(os.listdir(valid_image_dir)), len(os.listdir(test_image_dir))]\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x=[\"Train\", \"Valid\", \"Test\"], y=dataset_size)\n",
    "plt.xlabel(\"Dataset\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.title(\"Dataset Split\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(dataset_size, labels=[\"Train\", \"Valid\", \"Test\"], autopct=\"%1.1f%%\")\n",
    "plt.title('Dataset Split Percentage')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d2b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find train and valid duplicate images\n",
    "\n",
    "train_image_files = os.listdir(train_image_dir)\n",
    "valid_image_files = os.listdir(valid_image_dir)\n",
    "\n",
    "train_and_valid_duplicate_images = [file for file in train_image_files if file in valid_image_files]\n",
    "print(f\"Duplicate images: {train_and_valid_duplicate_images}\")\n",
    "print(f\"Duplicate images total length: {len(train_and_valid_duplicate_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91fb79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find train and test duplicate images\n",
    "\n",
    "train_image_files = os.listdir(train_image_dir)\n",
    "test_image_files = os.listdir(test_image_dir)\n",
    "\n",
    "train_and_test_duplicate_images = [file for file in train_image_files if file in test_image_files]\n",
    "print(f\"Duplicate images: {train_and_test_duplicate_images}\")\n",
    "print(f\"Duplicate images total length: {len(train_and_test_duplicate_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb182bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find test and valid duplicate images\n",
    "\n",
    "test_image_files = os.listdir(test_image_dir)\n",
    "valid_image_files = os.listdir(valid_image_dir)\n",
    "\n",
    "test_and_valid_duplicate_images = [file for file in test_image_files if file in valid_image_files]\n",
    "print(f\"Duplicate images: {test_and_valid_duplicate_images}\")\n",
    "print(f\"Duplicate images total length: {len(test_and_valid_duplicate_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find train and valid duplicate labels\n",
    "\n",
    "train_label_files = os.listdir(train_label_dir)\n",
    "valid_label_files = os.listdir(valid_label_dir)\n",
    "\n",
    "train_and_valid_duplicate_labels = [file for file in train_label_files if file in valid_label_files]\n",
    "print(f\"Duplicate labels: {train_and_valid_duplicate_labels}\")\n",
    "print(f\"Duplicate labels total length: {len(train_and_valid_duplicate_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21922982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find train and test duplicate labels\n",
    "\n",
    "train_label_files = os.listdir(train_label_dir)\n",
    "test_label_files = os.listdir(test_label_dir)\n",
    "\n",
    "train_and_test_duplicate_labels = [file for file in train_label_files if file in test_label_files]\n",
    "print(f\"Duplicate labels: {train_and_test_duplicate_labels}\")\n",
    "print(f\"Duplicate labels total length: {len(train_and_test_duplicate_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f4bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find test and valid duplicate labels\n",
    "\n",
    "test_label_files = os.listdir(test_label_dir)\n",
    "valid_label_files = os.listdir(valid_label_dir)\n",
    "\n",
    "test_and_valid_duplicate_labels = [file for file in test_label_files if file in valid_label_files]\n",
    "print(f\"Duplicate labels: {test_and_valid_duplicate_labels}\")\n",
    "print(f\"Duplicate labels total length: {len(test_and_valid_duplicate_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
